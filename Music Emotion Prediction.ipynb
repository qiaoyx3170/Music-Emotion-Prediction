{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook use different models to make predictions of emotions in music.  \n",
        "\n",
        "The models are trained on two datasets ([DEAM](https://cvml.unige.ch/databases/DEAM/), [EmoMusic](https://cvml.unige.ch/databases/emoMusic/)\n",
        "), with three types of deep embeddings ([AudioSet-VGGish](https://essentia.upf.edu/models.html#audioset-vggish), [Discogs-EffNet](https://essentia.upf.edu/models.html#discogs-effnet), and [MSD-MusiCNN](https://essentia.upf.edu/models.html#msd-musicnn)). Therefore, 6 models in total.  \n",
        "\n",
        "Then, we use Sptify's API to analysis the emotion prediction of the audio."
      ],
      "metadata": {
        "id": "dq3GeTdm0Ufa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "J3OEBc4RqfvQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6ea_G8Jpjhw"
      },
      "outputs": [],
      "source": [
        "!pip install essentia-tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from essentia import Pool\n",
        "from essentia.standard import (\n",
        "    MonoLoader,\n",
        "    TensorflowPredict,\n",
        "    TensorflowPredictMusiCNN,\n",
        ")"
      ],
      "metadata": {
        "id": "ltWbMTcSqCOp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load audio file"
      ],
      "metadata": {
        "id": "87c3AKPkqnXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an audio files.\n",
        "# The embeddings models work with input audio with the 16 KHz samplerate.\n",
        "audio_folder = \"./audio.002\"\n",
        "allAudios = os.walk(audio_folder)\n",
        "audio = []\n",
        "\n",
        "for dir, dirame, audiofile in sorted(allAudios):\n",
        "  for audioname in sorted(audiofile):\n",
        "    audio_path = os.path.join(dir, audioname)\n",
        "    print(audioname)\n",
        "\n",
        "    audio_resample = MonoLoader(filename=audio_path, sampleRate=16000)()    # resample audio to 16k Hz.\n",
        "    audio.append(audio_resample)\n",
        "    print(audio)"
      ],
      "metadata": {
        "id": "QdTzX5huqM8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting"
      ],
      "metadata": {
        "id": "vjLd4w4lSUXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEAM-AudioSet-VGGish"
      ],
      "metadata": {
        "id": "J8T980GeQE2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from essentia.standard import TensorflowPredictVGGish\n",
        "\n",
        "# Model files for inference of embeddings and arousal/valence.\n",
        "av_model_path = \"./essentia-models/deam-vggish-audioset-1/deam-vggish-audioset-1.pb\"\n",
        "pretrained_model_path = \"./essentia-models/audioset-vggish-3.pb\"\n",
        "\n",
        "# VGGish works in time domain, it doesn't need to specify patch_size and patch_hop_size,\n",
        "# output_layer name.\n",
        "output_layer = \"model/vggish/embeddings\"\n",
        "\n",
        "# Instantiate the embeddings model\n",
        "embeddings_model = TensorflowPredictVGGish(\n",
        "    graphFilename=pretrained_model_path,\n",
        "    output=output_layer,\n",
        ")"
      ],
      "metadata": {
        "id": "B-_b3i0ZUK3v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the arousal-valence model and run inference with TensorflowPredict().\n",
        "\n",
        "# Configure the input and output layers for this model.\n",
        "metadata = json.load(open(\"./essentia-models/deam-vggish-audioset-1/deam-vggish-audioset-1.json\", \"r\"))\n",
        "\n",
        "input_layer = metadata[\"schema\"][\"inputs\"][0][\"name\"]\n",
        "output_layer = metadata[\"schema\"][\"outputs\"][0][\"name\"]\n",
        "\n",
        "# Instantiate the arousal-valence model\n",
        "av_model = TensorflowPredict(\n",
        "    graphFilename=av_model_path,\n",
        "    inputs=[input_layer],\n",
        "    outputs=[output_layer],\n",
        ")"
      ],
      "metadata": {
        "id": "v140ef65YMD5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute embeddings.\n",
        "for i in audio:\n",
        "  embeddings = embeddings_model(i)\n",
        "  feature = embeddings.reshape(-1, 1, 1, embeddings.shape[1])\n",
        "  pool = Pool()\n",
        "  pool.set(input_layer, feature)\n",
        "  predictions = av_model(pool)[output_layer].squeeze()\n",
        "\n",
        "  # Estimate the average of the predictions to get an arousal-valence\n",
        "  print(f\"prediction: {predictions.mean(axis=0)}\")  # [valence, arousal]"
      ],
      "metadata": {
        "id": "Q-8gEZ9rYXZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17cfc883-30a8-4d1c-b0e0-28b7111ca75a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: [6.114302 6.247788]\n",
            "prediction: [5.8908854 5.915308 ]\n",
            "prediction: [5.951558  5.8261423]\n",
            "prediction: [5.610529  5.3558865]\n",
            "prediction: [5.47648  5.353496]\n",
            "prediction: [4.6646414 4.892181 ]\n",
            "prediction: [4.4502335 4.613028 ]\n",
            "prediction: [4.0791144 4.3315163]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEAM-Discogs-EffNet"
      ],
      "metadata": {
        "id": "w9TU65kNQckm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model files for inference of embeddings and arousal/valence.\n",
        "av_model_path = \"./essentia-models/deam-effnet-discogs-1/deam-effnet-discogs-1.pb\"\n",
        "embeddings_model_path = \"./essentia-models/effnet-discogs-1.pb\"\n",
        "\n",
        "patch_size = 128\n",
        "patch_hop_size = patch_size // 2\n",
        "\n",
        "input_layer = \"melspectrogram\"\n",
        "output_layer = \"onnx_tf_prefix_BatchNormalization_496/add_1\"\n",
        "\n",
        "# Instantiate the embeddings model\n",
        "embeddings_model = TensorflowPredictMusiCNN(\n",
        "    graphFilename=embeddings_model_path,\n",
        "    input=input_layer,\n",
        "    output=output_layer,\n",
        "    patchSize=patch_size,\n",
        "    patchHopSize=patch_hop_size,\n",
        ")"
      ],
      "metadata": {
        "id": "JlLN451yqQ9Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the arousal-valence model and run inference with TensorflowPredict().\n",
        "\n",
        "# Configure the input and output layers for this model.\n",
        "metadata = json.load(open(\"./essentia-models/deam-effnet-discogs-1/deam-effnet-discogs-1.json\", \"r\"))\n",
        "\n",
        "input_layer = metadata[\"schema\"][\"inputs\"][0][\"name\"]\n",
        "output_layer = metadata[\"schema\"][\"outputs\"][0][\"name\"]\n",
        "\n",
        "# Instantiate the arousal-valence model\n",
        "av_model = TensorflowPredict(\n",
        "    graphFilename=av_model_path,\n",
        "    inputs=[input_layer],\n",
        "    outputs=[output_layer],\n",
        ")"
      ],
      "metadata": {
        "id": "4LoV43s8YkkX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute embeddings.\n",
        "for i in audio:\n",
        "  embeddings = embeddings_model(i)\n",
        "\n",
        "  feature = embeddings.reshape(-1, 1, 1, embeddings.shape[1])\n",
        "  pool = Pool()\n",
        "  pool.set(input_layer, feature)\n",
        "  predictions = av_model(pool)[output_layer].squeeze()\n",
        "\n",
        "  # Estimate the average of the predictions to get an arousal-valence\n",
        "  print(f\"prediction: {predictions.mean(axis=0)}\")  # [valence, arousal]"
      ],
      "metadata": {
        "id": "htgErDlnYrSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb407f9d-c02e-44da-fc79-7e5ff5dc324f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: [6.271964  5.9751506]\n",
            "prediction: [6.7898927 5.7247977]\n",
            "prediction: [5.960185  5.8681226]\n",
            "prediction: [5.488956  4.9948626]\n",
            "prediction: [5.575601 5.563194]\n",
            "prediction: [5.054484 5.153976]\n",
            "prediction: [4.4691358 4.671598 ]\n",
            "prediction: [4.191529  4.4826646]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEAM-MSD-MusiCNN"
      ],
      "metadata": {
        "id": "AdJMvGW7Qhhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model files for inference of embeddings and arousal/valence.\n",
        "av_model_path = \"./essentia-models/deam-musicnn-msd-1/deam-musicnn-msd-1.pb\"\n",
        "pretrained_model_path = \"./essentia-models/msd-musicnn-1.pb\"\n",
        "\n",
        "\n",
        "patch_size = 187\n",
        "patch_hop_size = patch_size // 2\n",
        "\n",
        "input_layer = \"model/Placeholder\"\n",
        "output_layer = \"model/dense/BiasAdd\"\n",
        "\n",
        "# Instantiate the embeddings model\n",
        "embeddings_model = TensorflowPredictMusiCNN(\n",
        "    graphFilename=pretrained_model_path,\n",
        "    input=input_layer,\n",
        "    output=output_layer,\n",
        "    patchSize=patch_size,\n",
        "    patchHopSize=patch_hop_size,\n",
        ")"
      ],
      "metadata": {
        "id": "sn2-WpVuTrNV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the arousal-valence model and run inference with TensorflowPredict().\n",
        "\n",
        "# Configure the input and output layers for this model.\n",
        "metadata = json.load(open(\"./essentia-models/deam-musicnn-msd-1/deam-musicnn-msd-1.json\", \"r\"))\n",
        "\n",
        "input_layer = metadata[\"schema\"][\"inputs\"][0][\"name\"]\n",
        "output_layer = metadata[\"schema\"][\"outputs\"][0][\"name\"]\n",
        "\n",
        "# Instantiate the arousal-valence model\n",
        "av_model = TensorflowPredict(\n",
        "    graphFilename=av_model_path,\n",
        "    inputs=[input_layer],\n",
        "    outputs=[output_layer],\n",
        ")"
      ],
      "metadata": {
        "id": "aYSGl8wFY5pl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute embeddings.\n",
        "\n",
        "for i in audio:\n",
        "  embeddings = embeddings_model(i)\n",
        "\n",
        "  feature = embeddings.reshape(-1, 1, 1, embeddings.shape[1])\n",
        "  pool = Pool()\n",
        "  pool.set(input_layer, feature)\n",
        "  predictions = av_model(pool)[output_layer].squeeze()\n",
        "\n",
        "  # Estimate the average of the predictions to get an arousal-valence\n",
        "  print(f\"prediction: {predictions.mean(axis=0)}\")  # [valence, arousal]"
      ],
      "metadata": {
        "id": "WESp_rEaZBWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2c7bec-80f0-467b-badf-7fd6e38461fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: [5.6215873 5.9229913]\n",
            "prediction: [5.8505416 5.637046 ]\n",
            "prediction: [6.593474  6.1359262]\n",
            "prediction: [5.8707404 5.360821 ]\n",
            "prediction: [5.841827 5.894704]\n",
            "prediction: [5.2682815 5.4188595]\n",
            "prediction: [4.658526  4.7978835]\n",
            "prediction: [3.9800584 4.175116 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EmoMusic-AudioSet-VGGish"
      ],
      "metadata": {
        "id": "Pq49hRZDQfNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model files for inference of embeddings and arousal/valence.\n",
        "av_model_path = \"./essentia-models/emomusic-vggish-audioset-1/emomusic-vggish-audioset-1.pb\"\n",
        "pretrained_model_path = \"./essentia-models/audioset-vggish-3.pb\"\n",
        "\n",
        "# VGGish embeddings model works in time domain, it doesn't needs to specify patch_size and patch_hop_size,\n",
        "# only output_layer name.\n",
        "output_layer = \"model/vggish/embeddings\"\n",
        "\n",
        "# Instantiate the embeddings model\n",
        "embeddings_model = TensorflowPredictVGGish(\n",
        "    graphFilename=pretrained_model_path,\n",
        "    output=output_layer,\n",
        ")"
      ],
      "metadata": {
        "id": "sig1nh9JUobO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the arousal-valence model and run inference with TensorflowPredict().\n",
        "\n",
        "# Configure the input and output layers for this model.\n",
        "metadata = json.load(open(\"./essentia-models/emomusic-vggish-audioset-1/emomusic-vggish-audioset-1.json\", \"r\"))\n",
        "\n",
        "input_layer = metadata[\"schema\"][\"inputs\"][0][\"name\"]\n",
        "output_layer = metadata[\"schema\"][\"outputs\"][0][\"name\"]\n",
        "\n",
        "# Instantiate the arousal-valence model\n",
        "av_model = TensorflowPredict(\n",
        "    graphFilename=av_model_path,\n",
        "    inputs=[input_layer],\n",
        "    outputs=[output_layer],\n",
        ")"
      ],
      "metadata": {
        "id": "YpMxZR03ZHrh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute embeddings.\n",
        "for i in audio:\n",
        "  embeddings = embeddings_model(i)\n",
        "  feature = embeddings.reshape(-1, 1, 1, embeddings.shape[1])\n",
        "  pool.set(input_layer, feature)\n",
        "  predictions = av_model(pool)[output_layer].squeeze()\n",
        "\n",
        "  # Estimate the average of the predictions to get an arousal-valence\n",
        "  print(f\"prediction: {predictions.mean(axis=0)}\")  # [valence, arousal]"
      ],
      "metadata": {
        "id": "DUhclNqGZROG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52da5a17-7390-431d-ab3a-901b17b19520"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: [6.161597  6.0167737]\n",
            "prediction: [5.6637535 6.418614 ]\n",
            "prediction: [6.295356 6.213693]\n",
            "prediction: [4.951011  5.9048142]\n",
            "prediction: [5.5317736 5.681318 ]\n",
            "prediction: [4.5389795 4.650194 ]\n",
            "prediction: [5.1369176 4.438523 ]\n",
            "prediction: [4.5149903 4.1252203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EmoMusic-Discogs-EffNet"
      ],
      "metadata": {
        "id": "kHQeQKjDQwad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model files for inference of embeddings and arousal/valence.\n",
        "av_model_path = \"./essentia-models/emomusic-effnet-discogs-1/emomusic-effnet-discogs-1.pb\"\n",
        "embeddings_model_path = \"./essentia-models/effnet-discogs-1.pb\"\n",
        "\n",
        "patch_size = 128\n",
        "patch_hop_size = patch_size // 2\n",
        "\n",
        "input_layer = \"melspectrogram\"\n",
        "output_layer = \"onnx_tf_prefix_BatchNormalization_496/add_1\"\n",
        "\n",
        "# Instantiate the embeddings model\n",
        "embeddings_model = TensorflowPredictMusiCNN(\n",
        "    graphFilename=embeddings_model_path,\n",
        "    input=input_layer,\n",
        "    output=output_layer,\n",
        "    patchSize=patch_size,\n",
        "    patchHopSize=patch_hop_size,\n",
        ")"
      ],
      "metadata": {
        "id": "YecO4-j_Ud8Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the arousal-valence model and run inference with TensorflowPredict().\n",
        "\n",
        "# Configure the input and output layers for this model.\n",
        "metadata = json.load(open(\"./essentia-models/emomusic-effnet-discogs-1/emomusic-effnet-discogs-1.json\", \"r\"))\n",
        "\n",
        "input_layer = metadata[\"schema\"][\"inputs\"][0][\"name\"]\n",
        "output_layer = metadata[\"schema\"][\"outputs\"][0][\"name\"]\n",
        "\n",
        "# Instantiate the arousal-valence model\n",
        "av_model = TensorflowPredict(\n",
        "    graphFilename=av_model_path,\n",
        "    inputs=[input_layer],\n",
        "    outputs=[output_layer],\n",
        ")"
      ],
      "metadata": {
        "id": "0iisE2tAZZrQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute embeddings.\n",
        "for i in audio:\n",
        "  embeddings = embeddings_model(i)\n",
        "\n",
        "  feature = embeddings.reshape(-1, 1, 1, embeddings.shape[1])\n",
        "  pool = Pool()\n",
        "  pool.set(input_layer, feature)\n",
        "  predictions = av_model(pool)[output_layer].squeeze()\n",
        "\n",
        "  # Estimate the average of the predictions to get an arousal-valence\n",
        "  # representation for the entire song.\n",
        "  print(f\"prediction: {predictions.mean(axis=0)}\")  # [valence, arousal]"
      ],
      "metadata": {
        "id": "C7DfbbB7Zjy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3eabd5a-cb88-4339-eedf-9c2bef089cc5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: [7.0017533 6.36807  ]\n",
            "prediction: [6.1376977 6.7136407]\n",
            "prediction: [6.2747164 5.6342397]\n",
            "prediction: [4.7400765 5.74673  ]\n",
            "prediction: [5.5805    5.6627116]\n",
            "prediction: [5.074748 4.960502]\n",
            "prediction: [4.879995 4.312363]\n",
            "prediction: [4.668727  4.1893415]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EmoMusic-MSD-MusiCNN"
      ],
      "metadata": {
        "id": "wEntiRZTQ2Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model files for inference of embeddings and arousal/valence.\n",
        "av_model_path = \"./essentia-models/emomusic-musicnn-msd-1/emomusic-musicnn-msd-1.pb\"\n",
        "pretrained_model_path = \"./essentia-models/msd-musicnn-1.pb\"\n",
        "\n",
        "# Patch size and patch hop size different from Effnet.\n",
        "patch_size = 187\n",
        "patch_hop_size = patch_size // 2\n",
        "\n",
        "input_layer = \"model/Placeholder\"\n",
        "output_layer = \"model/dense/BiasAdd\"\n",
        "\n",
        "# Instantiate the embeddings model\n",
        "embeddings_model = TensorflowPredictMusiCNN(\n",
        "    graphFilename=pretrained_model_path,\n",
        "    input=input_layer,\n",
        "    output=output_layer,\n",
        "    patchSize=patch_size,\n",
        "    patchHopSize=patch_hop_size,\n",
        ")"
      ],
      "metadata": {
        "id": "38FGDEGuUjQU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the arousal-valence model and run inference with TensorflowPredict().\n",
        "\n",
        "# Configure the input and output layers for this model.\n",
        "metadata = json.load(open(\"./essentia-models/emomusic-musicnn-msd-1/emomusic-musicnn-msd-1.json\", \"r\"))\n",
        "\n",
        "input_layer = metadata[\"schema\"][\"inputs\"][0][\"name\"]\n",
        "output_layer = metadata[\"schema\"][\"outputs\"][0][\"name\"]\n",
        "\n",
        "# Instantiate the arousal-valence model\n",
        "av_model = TensorflowPredict(\n",
        "    graphFilename=av_model_path,\n",
        "    inputs=[input_layer],\n",
        "    outputs=[output_layer],\n",
        ")"
      ],
      "metadata": {
        "id": "YOMwXR30Ztdf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute embeddings.\n",
        "for i in audio:\n",
        "  embeddings = embeddings_model(i)\n",
        "\n",
        "  feature = embeddings.reshape(-1, 1, 1, embeddings.shape[1])\n",
        "  pool = Pool()\n",
        "  pool.set(input_layer, feature)\n",
        "  predictions = av_model(pool)[output_layer].squeeze()\n",
        "\n",
        "  # Estimate the average of the predictions to get an arousal-valence\n",
        "  print(f\"prediction: {predictions.mean(axis=0)}\")  # [valence, arousal]"
      ],
      "metadata": {
        "id": "U7sBgKBYZxxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9209de4e-5c44-455f-d11e-1cf4f384acdf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: [6.269694 5.799119]\n",
            "prediction: [5.071554 6.268912]\n",
            "prediction: [6.6763215 6.54575  ]\n",
            "prediction: [4.2763968 5.8142323]\n",
            "prediction: [5.871333 6.014273]\n",
            "prediction: [5.2285833 5.0768723]\n",
            "prediction: [5.3695736 4.3264213]\n",
            "prediction: [4.6982937 4.1428885]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spotify"
      ],
      "metadata": {
        "id": "HfwWdHjtcwPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "api_folder = \"./annotations-spotifyapi.002\"\n",
        "allAnnot = os.walk(api_folder, topdown=False)\n",
        "\n",
        "\n",
        "for dirs, dirames, apifile in sorted(allAnnot):\n",
        "  for api in sorted(apifile):\n",
        "    api_path = os.path.join(dirs, api)\n",
        "\n",
        "    metadata = yaml.safe_load(open(api_path, \"r\"))\n",
        "    valence = metadata[\"audio_features\"][\"valence\"]\n",
        "    arousal = metadata[\"audio_features\"][\"energy\"]\n",
        "    print([valence, arousal])"
      ],
      "metadata": {
        "id": "1s6_-ua9c3__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ee5752-792b-423f-d8d0-2a94a89aca89"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.955, 0.73]\n",
            "[0.228, 0.73]\n",
            "[0.176, 0.955]\n",
            "[0.365, 0.671]\n",
            "[0.469, 0.64]\n",
            "[0.748, 0.894]\n",
            "[0.573, 0.241]\n",
            "[0.461, 0.225]\n"
          ]
        }
      ]
    }
  ]
}